{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The SWAG Algorithm; a Mathematical Approach that Outperforms Traditional Deep Learning. Theory and Implementation\n",
    "\n",
    "\n",
    "\n",
    "##           solving any classification problems with just one layer neural network\n",
    "\n",
    "### **Saeid Safaei, PhD Candidate**\n",
    "#### 12/17/2018\n",
    "\n",
    "### 1. Introduction to SWAG**\n",
    "\n",
    "\n",
    "The link of Paper\n",
    "\n",
    "https://arxiv.org/pdf/1811.11813.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Suppose we have input P and Lable Y.\n",
    "####  There are some functions like F(x), G(X), H(x), T(X) which map input  P to output Y. The question is how to find those functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  Moreover, we can approximate a function with polynomials\n",
    "####  So, if we find a_{ij} then the problem solved. How find those coefficients?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2.png\" width=\"45%\" height=\"45%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  We push the inputs to the formula. Now we have the following.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"3.png\" width=\"60%\" height=\"60%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  If we solve the following problem, we can find those coefficients. P and Y are constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"4.png\"  width=\"50%\" height=\"50%\" ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Traditional deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"5.png\" ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Our model with 1 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"6.png\" ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Our model with 2 Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"7.png\" ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BRG_user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import read_csv\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import RandomNormal,glorot_normal\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "np.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Run the code you need to download this file. it is the implementation of SWAG Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Dense_SWAG import Dense_Co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define normalize Functions\n",
    "\n",
    "### Define Rescaling Functions from 0-1 to 0.1-0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define normalize Functions\n",
    "def normalize(d):\n",
    "    # d is a (n x dimension) np array\n",
    "    d -= np.min(d, axis=0)\n",
    "    d /= np.ptp(d, axis=0)\n",
    "    return d\n",
    "\n",
    "#Define Rescaling Functions from 0-1 to 0.1-0.9\n",
    "def rescale_range(d):\n",
    "    # d is a (n x dimension) np array\n",
    "    d=np.multiply(d, 0.89)\n",
    "    d=np.add(d, 0.01)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method compiles a Keras model and return it.\n",
    "### input_dim is the dimension of input and  output_dim is the dimension of output.\n",
    "### In our model image the number of Green, Red, and Blue are equal, and we show them by hidden_dim\n",
    "### by default hidden_dim = input_dim but we can change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_v1(input_dim,hidden_dim,output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense_Co(output_dim,hidden_dim=hidden_dim ,input_dim=input_dim))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(input_dim,hidden_dim,output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense_Co(output_dim,hidden_dim=hidden_dim ,input_dim=input_dim,  kernel_initializer=RandomNormal(\n",
    "            mean=0.0, stddev=0.04), bias_initializer=RandomNormal(mean=0.0, stddev=0.04)))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=5\n",
    "batch_size=1\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "cvscores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ========================================================\n",
    "# First experiment\n",
    "##  Load data of   ionosphere data set\n",
    "\n",
    "\n",
    "This is a small dataset that you can <a href=\"https://www.kaggle.com/creepyghost/uci-ionosphere/version/1\">download from the kaggle </a>.\n",
    "\n",
    "\n",
    "### About this file:\n",
    "\n",
    "Source: Donated to UCI Machine Learning Repository by\n",
    "\n",
    "Donor:\n",
    "\n",
    "Vince Sigillito (vgs '@' aplcen.apl.jhu.edu)\n",
    "\n",
    "Source:\n",
    "\n",
    "Space Physics Group Applied Physics Laboratory Johns Hopkins University Johns Hopkins Road Laurel, MD 20723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data\\ionosphere_data_kaggle.csv\", header=1)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#Number of Rows and columns\n",
    "Number_rows,Input_size=dataset.shape\n",
    "\n",
    "X = dataset[:,0:Input_size-1].astype(float)\n",
    "Y = dataset[:,Input_size-1]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use 10 k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_1 (Dense_Co)       (None, 1)                 7345      \n",
      "=================================================================\n",
      "Total params: 7,345\n",
      "Trainable params: 7,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 88.89%\n",
      "acc: 97.22%\n",
      "acc: 97.22%\n",
      "acc: 97.22%\n",
      "acc: 97.14%\n",
      "acc: 97.14%\n",
      "acc: 97.06%\n",
      "acc: 97.06%\n",
      "acc: 97.06%\n",
      "acc: 97.06%\n",
      "The Mean accuracy is 96.31% (+/- 2.47%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "\n",
    "# create model\n",
    "model=build_model(Input_size-1,Input_size-1,1)\n",
    "model.summary()\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ========================================================\n",
    "# Second experiment\n",
    "##  Load data of   Iris Data Set\n",
    "\n",
    "\n",
    "This is a small dataset that you can <a href=\"https://www.kaggle.com/uciml/iris#Iris.csv\">download from the kaggle </a>.\n",
    "\n",
    "\n",
    "### About this file:\n",
    "\n",
    "https://www.kaggle.com/uciml/iris#Iris.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data\\iris.csv\", header=1)\n",
    "dataset = dataframe.values\n",
    "\n",
    "\n",
    "#Number of Rows and columns\n",
    "Number_rows,Input_size=dataset.shape\n",
    "\n",
    "X = dataset[:,0:Input_size-1].astype(float)\n",
    "Y = dataset[:,Input_size-1]\n",
    "X=normalize(X)\n",
    "X=rescale_range(X)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_2 (Dense_Co)       (None, 1)                 211       \n",
      "=================================================================\n",
      "Total params: 211\n",
      "Trainable params: 211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 93.33%\n",
      "acc: 93.33%\n",
      "acc: 93.33%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 93.33%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "The Mean accuracy is 97.33% (+/- 3.27%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "\n",
    "# create model\n",
    "model=build_model(Input_size-1,Input_size-1,1)\n",
    "model.summary()\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ========================================================\n",
    "# Third experiment\n",
    "##  Load data of  banknote authentication Data Set \n",
    "\n",
    "This is a small dataset that you can <a href=\"https://www.kaggle.com/jacksonharper/data_banknote_authentication\">download from the kaggle </a>.\n",
    "\n",
    "\n",
    "### About this file:\n",
    "\n",
    "https://www.kaggle.com/jacksonharper/data_banknote_authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data\\data_banknote_authentication.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "\n",
    "#Number of Rows and columns\n",
    "Number_rows,Input_size=dataset.shape\n",
    "\n",
    "X = dataset[:,0:Input_size-1].astype(float)\n",
    "Y = dataset[:,Input_size-1]\n",
    "X=normalize(X)\n",
    "X=rescale_range(X)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_3 (Dense_Co)       (None, 1)                 145       \n",
      "=================================================================\n",
      "Total params: 145\n",
      "Trainable params: 145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 96.38%\n",
      "acc: 94.93%\n",
      "acc: 98.54%\n",
      "acc: 97.81%\n",
      "acc: 95.62%\n",
      "acc: 97.81%\n",
      "acc: 98.54%\n",
      "acc: 99.27%\n",
      "acc: 94.16%\n",
      "acc: 98.54%\n",
      "The Mean accuracy is 97.16% (+/- 1.67%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "\n",
    "# create model\n",
    "model=build_model(Input_size-1,Input_size-1,1)\n",
    "model.summary()\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ========================================================\n",
    "# Fourth experiment\n",
    "##  Load data of  Connectionist Bench (Sonar)\n",
    "\n",
    "This is a small dataset that you can <a href=\"https://www.kaggle.com/adx891/sonar-data-set\">download from the kaggle </a>.\n",
    "\n",
    "\n",
    "### About this file:\n",
    "\n",
    "Connectionist Bench (Sonar, Mines vs. Rocks) Data Set Download: Data Folder, Data Set Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data\\sonar.all-data.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "\n",
    "#Number of Rows and columns\n",
    "Number_rows,Input_size=dataset.shape\n",
    "\n",
    "X = dataset[:,0:Input_size-1].astype(float)\n",
    "Y = dataset[:,Input_size-1]\n",
    "X=normalize(X)\n",
    "X=rescale_range(X)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_4 (Dense_Co)       (None, 1)                 22321     \n",
      "=================================================================\n",
      "Total params: 22,321\n",
      "Trainable params: 22,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 81.82%\n",
      "acc: 85.71%\n",
      "acc: 90.48%\n",
      "acc: 95.24%\n",
      "acc: 100.00%\n",
      "acc: 95.24%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "The Mean accuracy is 94.85% (+/- 6.37%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "\n",
    "# create model\n",
    "model=build_model(Input_size-1,Input_size-1,1)\n",
    "model.summary()\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  ========================================================\n",
    "# Fifth  experiment    \n",
    "##  Load data of  Pima Indians Diabetes Database\n",
    "\n",
    "\n",
    "\n",
    "This is a small dataset that you can <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">download from the kaggle </a>.\n",
    "\n",
    "\n",
    "### About this file:\n",
    "\n",
    "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data\\diabetes.csv\", header=1)\n",
    "dataset = dataframe.values\n",
    "\n",
    "\n",
    "#Number of Rows and columns\n",
    "Number_rows,Input_size=dataset.shape\n",
    "\n",
    "X = dataset[:,0:Input_size-1].astype(float)\n",
    "Y = dataset[:,Input_size-1]\n",
    "X=normalize(X)\n",
    "X=rescale_range(X)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_5 (Dense_Co)       (None, 1)                 481       \n",
      "=================================================================\n",
      "Total params: 481\n",
      "Trainable params: 481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 68.83%\n",
      "acc: 77.92%\n",
      "acc: 80.52%\n",
      "acc: 83.12%\n",
      "acc: 83.12%\n",
      "acc: 79.22%\n",
      "acc: 75.32%\n",
      "acc: 73.68%\n",
      "acc: 75.00%\n",
      "acc: 78.95%\n",
      "The Mean accuracy is 77.57% (+/- 4.22%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "\n",
    "# create model\n",
    "model=build_model(Input_size-1,Input_size-1,1)\n",
    "model.summary()\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  ========================================================\n",
    "# Sixth  experiment        \n",
    "##  Load data of  Red Wine Quality\n",
    "\n",
    "\n",
    "This is a small dataset that you can download \n",
    "<a href=\"https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009#winequality-red.csv\">\n",
    "Here </a>.\n",
    "\n",
    "\n",
    "### About this file:\n",
    "\n",
    "This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data\\winequality-red.csv\", header=1)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#Number of Rows and columns\n",
    "Number_rows,Input_size=dataset.shape\n",
    "\n",
    "X = dataset[:,0:Input_size-1].astype(float)\n",
    "Y = dataset[:,Input_size-1]\n",
    "X=normalize(X)\n",
    "X=rescale_range(X)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_6 (Dense_Co)       (None, 1)                 859       \n",
      "=================================================================\n",
      "Total params: 859\n",
      "Trainable params: 859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 58.39%\n",
      "acc: 54.66%\n",
      "acc: 59.63%\n",
      "acc: 60.62%\n",
      "acc: 53.12%\n",
      "acc: 63.12%\n",
      "acc: 58.75%\n",
      "acc: 60.62%\n",
      "acc: 51.90%\n",
      "acc: 59.87%\n",
      "The Mean accuracy is 58.07% (+/- 3.45%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "\n",
    "# create model\n",
    "model=build_model(Input_size-1,Input_size-1,1)\n",
    "model.summary()\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========================================================\n",
    "## Seventh  experiment\n",
    "Load data of THE MNIST DATABASE of handwritten digits\n",
    "\n",
    "### About this file:\n",
    "\n",
    "    \n",
    "The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs= 4\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train+10\n",
    "x_test = x_test+10\n",
    "\n",
    "x_train /= 300\n",
    "x_test /= 300\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "number_train=60000 \n",
    "number_test=10000\n",
    "\n",
    "x_train=x_train[0:number_train,:]\n",
    "x_test=x_test[0:number_test,:]\n",
    "\n",
    "y_train=y_train[0:number_train]\n",
    "y_test=y_test[0:number_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense__co_7 (Dense_Co)       (None, 10)                2385010   \n",
      "=================================================================\n",
      "Total params: 2,385,010\n",
      "Trainable params: 2,385,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0205 - acc: 0.9431 - val_loss: 0.0134 - val_acc: 0.9696\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0117 - acc: 0.9772 - val_loss: 0.0113 - val_acc: 0.9751\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0097 - acc: 0.9832 - val_loss: 0.0106 - val_acc: 0.9766\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0084 - acc: 0.9875 - val_loss: 0.0097 - val_acc: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff1e9696d8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models=build_model(784,500,10)\n",
    "models.summary()\n",
    "models.fit(x_train, y_train,validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
